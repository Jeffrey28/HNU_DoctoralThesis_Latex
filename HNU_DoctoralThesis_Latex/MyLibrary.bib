@article{2017_qinxiaohui_FeiYunZhiCheLiangDuiLieDeFenBuShiKongZhi,
  title = {非匀质车辆队列的分布式控制},
  author = {{秦晓辉} and {王建强} and {谢伯元} and {胡满江} and {李克强}},
  year = {2017},
  journal = {汽车工程},
  volume = {39},
  number = {1},
  pages = {73--78},
  abstract = {提出了对称通信拓扑下具有不同参数摄动的非匀质车辆队列鲁棒稳定性分析方法和分布式控制器设计方法.通过反馈线性化技术求得队列节点的线性动力学响应,结合分布式控制...},
  annotation = {00000},
  file = {/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/V2LLVB8M/秦晓辉_2017_汽车工程_非匀质车辆队列的分布式控制.pdf}
}

@article{2020_aradi_Survey,
  title = {Survey of {{Deep Reinforcement Learning}} for {{Motion Planning}} of {{Autonomous Vehicles}}},
  author = {Aradi, Szil{\'a}rd},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.11231 [cs, eess, stat]},
  eprint = {2001.11231},
  primaryclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/2001.11231},
  abstract = {Academic research in the field of autonomous vehicles has reached high popularity in recent years related to several topics as sensor technologies, V2X communications, safety, security, decision making, control, and even legal and standardization rules. Besides classic control design approaches, Artificial Intelligence and Machine Learning methods are present in almost all of these fields. Another part of research focuses on different layers of Motion Planning, such as strategic decisions, trajectory planning, and control. A wide range of techniques in Machine Learning itself have been developed, and this article describes one of these fields, Deep Reinforcement Learning (DRL). The paper provides insight into the hierarchical motion planning problem and describes the basics of DRL. The main elements of designing such a system are the modeling of the environment, the modeling abstractions, the description of the state and the perception models, the appropriate rewarding, and the realization of the underlying neural network. The paper describes vehicle models, simulation possibilities and computational requirements. Strategic decisions on different layers and the observation models, e.g., continuous and discrete state representations, grid-based, and camera-based solutions are presented. The paper surveys the state-of-art solutions systematized by the different tasks and levels of autonomous driving, such as car-following, lane-keeping, trajectory following, merging, or driving in dense traffic. Finally, open questions and future challenges are discussed.},
  archiveprefix = {arxiv},
  keywords = {6.综述/Thesis,7.已读✅,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/I4GQCX8J/Aradi_2020_arXiv2001.11231 [cs, eess, stat]_Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles.pdf}
}

@article{2021_chen_Graph,
  title = {Graph Neural Network and Reinforcement Learning for Multi-Agent Cooperative Control of Connected Autonomous Vehicles},
  author = {Chen, Sikai and Dong, Jiqian and Ha, Paul (Young Joun) and Li, Yujie and Labi, Samuel},
  year = {2021},
  journal = {Computer-Aided Civil and Infrastructure Engineering},
  volume = {36},
  number = {7},
  pages = {838--857},
  issn = {1467-8667},
  doi = {10.1111/mice.12702},
  urldate = {2021-07-09},
  abstract = {A connected autonomous vehicle (CAV) network can be defined as a set of connected vehicles including CAVs that operate on a specific spatial scope that may be a road network, corridor, or segment. The spatial scope constitutes an environment where traffic information is shared and instructions are issued for controlling the CAVs movements. Within such a spatial scope, high-level cooperation among CAVs fostered by joint planning and control of their movements can greatly enhance the safety and mobility performance of their operations. Unfortunately, the highly combinatory and volatile nature of CAV networks due to the dynamic number of agents (vehicles) and the fast-growing joint action space associated with multi-agent driving tasks pose difficultly in achieving cooperative control. The problem is NP-hard and cannot be efficiently resolved using rule-based control techniques. Also, there is a great deal of information in the literature regarding sensing technologies and control logic in CAV operations but relatively little information on the integration of information from collaborative sensing and connectivity sources. Therefore, we present a novel deep reinforcement learning-based algorithm that combines graphic convolution neural network with deep Q-network to form an innovative graphic convolution Q network that serves as the information fusion module and decision processor. In this study, the spatial scope we consider for the CAV network is a multi-lane road corridor. We demonstrate the proposed control algorithm using the application context of freeway lane-changing at the approaches to an exit ramp. For purposes of comparison, the proposed model is evaluated vis-\`a-vis traditional rule-based and long short-term memory-based fusion models. The results suggest that the proposed model is capable of aggregating information received from sensing and connectivity sources and prescribing efficient operative lane-change decisions for multiple CAVs, in a manner that enhances safety and mobility. That way, the operational intentions of individual CAVs can be fulfilled even in partially observed and highly dynamic mixed traffic streams. The paper presents experimental evidence to demonstrate that the proposed algorithm can significantly enhance CAV operations. The proposed algorithm can be deployed at roadside units or cloud platforms or other centralized control facilities.},
  langid = {english},
  keywords = {/reading},
  file = {/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/4IFJFG4I/Chen_2021_Computer-Aided Civil and Infrastructure Engineering_Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles.pdf;/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/UHT54WY5/mice.html}
}

@article{2021_di_survey,
  title = {A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy: {{From}} Physics-Based to {{AI-guided}} Driving Policy Learning},
  shorttitle = {A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy},
  author = {Di, Xuan and Shi, Rongye},
  year = {2021},
  month = apr,
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {125},
  pages = {103008},
  issn = {0968-090X},
  doi = {10.1016/j.trc.2021.103008},
  urldate = {2021-03-18},
  abstract = {This paper serves as an introduction and overview of the potentially useful models and methodologies from artificial intelligence (AI) into the field of transportation engineering for autonomous vehicle (AV) control in the era of mixed autonomy when AVs drive alongside human-driven vehicles (HV). It is the first-of-its-kind survey paper to comprehensively review literature in both transportation engineering and AI for mixed traffic modeling. We will discuss state-of-the-art applications of AI-guided methods, identify opportunities and obstacles, and raise open questions. We divide the stage of AV deployment into four phases: the pure HVs, the HV-dominated, the AV-dominated, and the pure AVs. This paper is primarily focused on the latter three phases. Models used for each phase are summarized, encompassing game theory, deep (reinforcement) learning, and imitation learning. While reviewing the methodologies, we primarily focus on the following research questions: (1) What scalable driving policies are to control a large number of AVs in mixed traffic comprised of human drivers and uncontrollable AVs? (2) How do we estimate human driver behaviors? (3) How should the driving behavior of uncontrollable AVs be modeled in the environment? (4) How are the interactions between human drivers and autonomous vehicles characterized? We also provide a list of public datasets and simulation software related to AVs. Hopefully this paper will not only inspire our transportation community to rethink the conventional models that are developed in the data-shortage era, but also start conversations with other disciplines, in particular robotics and machine learning, to join forces towards creating a safe and efficient mixed traffic ecosystem.},
  copyright = {8.795},
  langid = {english},
  lccn = {9.02},
  keywords = {/reading,1.重要文献,6.综述/Thesis,7.已读✅,Artificial intelligence (AI),Autonomous vehicle (AV) control,Computer Science - Artificial Intelligence,Computer Science - Robotics,Mixed autonomy},
  file = {/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/48NIF842/Di_2021_Transportation Research Part C Emerging Technologies_A survey on autonomous vehicle control in the era of mixed-autonomy.pdf;/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/N96BQD3Y/Di_2020_arXiv2007.05156 [cs]_A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy.pdf;/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/6LUXY4BI/S0968090X21000401.html;/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/86EA48G5/2007.html}
}

@phdthesis{2021_qiao_Reinforcement,
  title = {Reinforcement {{Learning}} for {{Behavior Planning}} of {{Autonomous Vehicles}} in {{Urban Scenarios}}},
  author = {Qiao, Zhiqian},
  year = {2021},
  address = {{United States -- Pennsylvania}},
  url = {https://www.proquest.com/docview/2572558029/abstract/463A5C2CFD9F44E0PQ/1},
  urldate = {2022-01-18},
  abstract = {How autonomous vehicles and human drivers share public transportation systems is an important problem, as fully automatic transportation environments are still a long way off. Behavioral decision making serves as a key link in autonomous driving technology. Within conventional self-driving technology, heuristic-based rules-enumeration methods fulfill major tasks for behavioral decision making. However, for such a complex behavior as driving, the development of a suitable set of rules is a laborious engineering task that does not guarantee an optimal policy. Reinforcement learning (RL) is a decision-making method with strong recent successes that is capable of solving for an optimal policy, and can map diverse observations to actions in a variety of complex situations. However, RL has its own problems, such as exceptionally long training times, unstable training results and difficult reward tuning. In this thesis, we present a series of behavior planning structures and algorithms that are based on the advantages coming from both reinforcement learning and heuristic-based rules-enumeration. The resultant contributions include: 1. Creation of an Automatically Generated Curriculum in order to increase the learning speed for RL. 2. Improvement of the policy network of RL with an LSTM module in order to get better performance on a given task. 3. Creation of a hierarchical RL structure with hybrid reward mechanism which can accomplish the behavior decision procedure with the help of heuristic-based methods. 4. Application of the hierarchical RL structure to a comprehensive range of urban intersection scenarios, to include approaching, observation, and traversing. Compared to traditional heuristic-based rules-enumeration methods, which need a large amount of effort to design rules which can cover as many scenarios as possible, reinforcement learning can help to learn such an optimal policy automatically. On the other hand, our algorithm can help RL to be more sample-efficient and converges to an optimal policy faster than competing algorithms.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798538168347},
  langid = {english},
  school = {Carnegie Mellon University},
  keywords = {6.综述/Thesis,8.要读},
  file = {/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/SI8N6YS4/Qiao_2021_Reinforcement Learning for Behavior Planning of Autonomous Vehicles in Urban Scenarios.pdf}
}
